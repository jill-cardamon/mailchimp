input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["nginx"]
    auto_offset_reset => "earliest"
    decorate_events => "extended"
  }
}

filter {
  mutate {
    # remove metadata fields and [event][original] field that contains the raw json log
    remove_field => ["[event][original]", "@version"]
  }

  # add static fields
  mutate {
    add_field => {"[fields][region]" => "us-west-1"}
    add_field => {"[fields][assetid]" => "8972349837489237"}
  }

  if [@metadata][kafka][topic] == "nginx" {
    mutate {
      add_field => [ "[sourcetype]", "nginx" ]
      add_field => [ "[index]", "nginx" ]
    }
  }

  json {
    # parse json and remove raw message
    # note: consider adding some logic here to handle the case when json isn't parsed successfully
    source => "message"
    target => "event"
    remove_field => ["message"]
  }

  date {
    # set the @timestamp field to match the log time for Opensearch dashboard filtering 
    match => ["[event][time]", "dd/MMM/yyyy:HH:mm:ss Z"]
    target => "@timestamp"
    add_field => { "debug" => "timestampMatched"}
  }

  ruby {
    # derive epoch time from the time field in the event
    code => "event.set('time', DateTime.strptime(event.get('[event][time]'), '%d/%b/%Y:%T %z').strftime('%s').to_i)" 
  }

  mutate {
    # remove time from the event
    remove_field => ["[event][time]"]
  }
}

output {
  opensearch {        
    hosts  => ["opensearch-node1:9200"]     
    auth_type => {            
        type => 'basic'           
        user => 'admin'           
        password => "${OPENSEARCH_INITIAL_ADMIN_PASSWORD}"           
    }
    ssl_certificate_verification => false
    index => "nginx"
    action => "create"
  }    

  # for development purposes. remove to avoid cluttered logstash logs.
  stdout {
    codec => rubydebug
  }
}