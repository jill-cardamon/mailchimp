input {
 kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["nginx_access_logs"]
    auto_offset_reset => "earliest"
 }
}

filter {
  mutate {
    # remove metadata fields and [event][original] field that contains the raw json log
    remove_field => ["[event][original]", "@version", "@timestamp"]
  }

  mutate {
    # add static fields
    add_field => {"[fields][region]" => "us-west-1"}
    add_field => {"[fields][assetid]" => "8972349837489237"}
  }

  json {
    # parse json and remove raw message
    # note: message will remain if json is not successfully parsed
    # TODO: consider adding some sort of flag here if json isn't parsed successfully
    source => "message"
    target => "event"
    remove_field => ["message"]
  }

  mutate {
    copy => { "[event][time]" => "[time]" }
  }

  ruby {
    # derive epoch time from the time field in the event
    code => "event.set('time', DateTime.strptime(event.get('time'), '%d/%b/%Y:%T %z').strftime('%s'))" 
  }

  mutate {
    # remove metadata fields and [event][original] field that contains the raw json log
    remove_field => ["[event][time]"]
  }
}

output {
  opensearch {        
    hosts  => ["opensearch-node1:9200"]     
    auth_type => {            
        type => 'basic'           
        user => 'admin'           
        password => "${OPENSEARCH_INITIAL_ADMIN_PASSWORD}"           
    }
    ssl_certificate_verification => false
    index => "nginx_access_logs"
    action => "create"
  }    

  stdout {
    codec => rubydebug
  }
}